name: Generate Sitemap

on:
  push:

jobs:
  get_articles:
    runs-on: ubuntu-latest
    timeout-minutes: 10
    outputs:
      articles: ${{ steps.sorted_articles.outputs.articles }}
    steps:
      - id: sorted_articles
        run: |
          set -e
          fetch_url=https://render-sample-1.onrender.com/articles
          example_url=https://example.com
          page=1
          per_page=3
          res=$(curl -fSsL -H "x-api-key: ${{ secrets.X_API_KEY }}" "${fetch_url}?page=${page}&per_page=${per_page}")
          articles=$(echo $res | jq --arg example_url "$example_url" -c '.articles[] | [{url:"\($example_url)/\(.route)/\(.slug)/", refreshed_at: .refreshed_at}]')
          total=$(echo $res | jq -r '.total')
          max_page=$((($total + $per_page - 1) / $per_page))
          for ((page=2; page<=max_page; page++)); do
            res=$(curl -fSsL -H "x-api-key: ${{ secrets.X_API_KEY }}" "${fetch_url}?page=${page}&per_page=${per_page}")
            tmp_artices=$(echo $res | jq --arg example_url "$example_url" -c '.articles[] | [{url:"\($example_url)/\(.route)/\(.slug)/", refreshed_at: .refreshed_at}]')
            articles=$(echo $articles $tmp_artices | jq -s -c add)
          done
          articles=$(echo $articles | jq -c 'sort_by(.refreshed_at) | reverse')
          echo "articles=${articles}" >> "$GITHUB_OUTPUT"
  get_users:
    runs-on: ubuntu-latest
    timeout-minutes: 10
    outputs:
      users: ${{ steps.sorted_users.outputs.users }}
    steps:
      - id: sorted_users
        run: |
          set -e
          fetch_url=https://render-sample-1.onrender.com/users
          example_url=https://example.com
          page=1
          per_page=3
          res=$(curl -fSsL -H "x-api-key: ${{ secrets.X_API_KEY }}" "${fetch_url}?page=${page}&per_page=${per_page}")
          users=$(echo $res | jq '.users |= map(select(.articles | length > 0))' | jq --arg example_url "$example_url" -c '.users[] | [{url:"\($example_url)/users/\(.id)/", refreshed_at: .refreshed_at}]')
          # users=$(echo $res | jq --arg example_url "$example_url" -c '.users[] | [{url:"\($example_url)/users/\(.id)/", refreshed_at: .refreshed_at, articles: .articles}]')
          total=$(echo $res | jq -r '.total')
          max_page=$((($total + $per_page - 1) / $per_page))
          for ((page=2; page<=max_page; page++)); do
            res=$(curl -fSsL -H "x-api-key: ${{ secrets.X_API_KEY }}" "${fetch_url}?page=${page}&per_page=${per_page}")
            tmp_users=$(echo $res | jq '.users |= map(select(.articles | length > 0))' | jq --arg example_url "$example_url" -c '.users[] | [{url:"\($example_url)/users/\(.id)/", refreshed_at: .refreshed_at}]')
            # tmp_users=$(echo $res | jq --arg example_url "$example_url" -c '.users[] | [{url:"\($example_url)/users/\(.id)/", refreshed_at: .refreshed_at, articles: .articles}]')
            users=$(echo $users $tmp_users | jq -s -c add)
          done
          users=$(echo $users | jq -c 'sort_by(.refreshed_at) | reverse')
          echo "users=${users}" >> "$GITHUB_OUTPUT"
  generate_sitemap:
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs:
      - get_articles
      - get_users
    steps:
      - name: generate sitemap
        run: |
          article_urls=$(echo ${{ toJSON(needs.get_articles.outputs.articles) }} | jq '.[] | .url')
          users_urls=$(echo ${{ toJSON(needs.get_users.outputs.users) }} | jq '.[] | .url')
          echo $article_urls
          echo $users_urls
